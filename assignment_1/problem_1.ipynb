{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/pidipidi/cs470_IAI/blob/main/assignment_1/mlp_solution_2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEXqCDmc4f6Y"
   },
   "source": [
    "# **Problem1: Implementing a Multi-layer Perceptron (MLP)**\n",
    "\n",
    "#### In this exercise, you will implement a neural network with fully-connected layers to perform image classification, and test it out on the [STL-10](https://cs.stanford.edu/~acoates/stl10/) dataset. Please, run following blocks for running your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rK9eUV4V-XaU"
   },
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9fNcMT74mZX"
   },
   "source": [
    "## Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "train_dataset = datasets.STL10('data', split='train', download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.STL10('data', split='test', download=True, transform=transforms.ToTensor()) # we use the test set as a validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rK9eUV4V-XaU"
   },
   "source": [
    "#### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6f4W6thRw8HA",
    "outputId": "f0c56515-9a3d-47ad-d8b6-c78e5203030e"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=True)\n",
    "val_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=True)\n",
    "\n",
    "X_train, y_train = next(iter(train_loader)) # -1, 96, 96, 3\n",
    "X_train = np.array([transforms.ToPILImage()(x) for x in X_train])\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_val, y_val = next(iter(val_loader))\n",
    "X_val   = np.array([transforms.ToPILImage()(x) for x in X_val])\n",
    "y_val   = np.array(y_val)\n",
    "\n",
    "X_train = X_train.transpose(0,3,1,2).astype(\"float\") # -1, 3, 96, 96\n",
    "X_val   = X_val.transpose(0,3,1,2).astype(\"float\")\n",
    "\n",
    "# Normalize the data: subtract the mean image\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "X_train -= mean_image\n",
    "X_val -= mean_image\n",
    "\n",
    "# Reshape the data\n",
    "X_train = X_train.reshape(len(X_train), -1)\n",
    "X_val = X_val.reshape(len(X_val), -1)\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rK9eUV4V-XaU"
   },
   "source": [
    "### Visualize 10 different classes of images in the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"airplane\", \"bird\", \"car\", \"cat\", \"deer\", \"dog\", \"horse\", \"monkey\", \"ship\", \"truck\"]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Training dataset\n",
    "X, y = next(iter(train_loader))\n",
    "      \n",
    "# visualize training image for each class\n",
    "sample_images = [X[np.asarray(y) == label][0] for label in range(10)]\n",
    "\n",
    "# show images\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
    "i = 0\n",
    "for row in axes:\n",
    "  for axis in row:\n",
    "    axis.set_xticks([])\n",
    "    axis.set_yticks([])\n",
    "    axis.set_xlabel(classes[i], fontsize=15)\n",
    "    axis.imshow(sample_images[i].permute(1, 2, 0))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rK9eUV4V-XaU"
   },
   "source": [
    "## Create a model class\n",
    "\n",
    "In this work, we use a neural network with fully-connected layers in which each layer is represented as a function: $f(\\mathbf{x}) = \\mathbf{W}\\mathbf{x} + \\mathbf{b}$, where $\\mathbf{W}$ is a weight matrix, $\\mathbf{x}$ is its input, and $\\mathbf{b}$ is a bias. The activation function of the first layer is the ReLU function: $\\sigma(\\mathbf{x}) = \\max(0, \\mathbf{x})$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GdQWZt82E1zB"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "  return 1./(1.+np.exp(-x))\n",
    "\n",
    "class MLP(object):\n",
    "  \"\"\"\n",
    "  A multi-layer fully-connected neural network has an input dimension of\n",
    "  d, a hidden layer dimension of h, and performs classification over c classes.\n",
    "  You must train the network with a softmax loss function and L1 regularization on the\n",
    "  weight matrices. The network uses a ReLU/LeakyReLU/etc nonlinearity after the first fully\n",
    "  -connected layer.\n",
    "\n",
    "  The network has the following architecture:\n",
    "\n",
    "  Input - Linear layer - ReLU/LeakyReLU/etc - Linear layer - Softmax\n",
    "\n",
    "  The outputs of the network are the labels for each class.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, input_size, hidden_size, output_size, activation, std=1e-4):\n",
    "    \"\"\"\n",
    "    An initialization function\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size: integer\n",
    "        the dimension d of the input data.\n",
    "    hidden_size: integer\n",
    "        the number of neurons h in the hidden layer.\n",
    "    output_size: integer\n",
    "        the number of classes c.\n",
    "    activation: string\n",
    "        activation method name\n",
    "    std: float\n",
    "        standard deviation\n",
    "    \"\"\"\n",
    "    # w1: weights for the first linear layer\n",
    "    # b1: biases for the first linear layer\n",
    "    # w2: weights for the second linear layer\n",
    "    # b2: biases for the second linear layer\n",
    "\n",
    "    self.params = {}\n",
    "    self.params['w1'] = std * np.random.randn(input_size, hidden_size)\n",
    "    self.params['b1'] = np.zeros(hidden_size)\n",
    "    self.params['w2'] = std * np.random.randn(hidden_size, output_size)\n",
    "    self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    self.leaky_relu_c = 0.01\n",
    "    self.selu_lambda  = 1.05\n",
    "    self.selu_alpha   = 1.67\n",
    "    self.activation_method = ['ReLU','LeakyReLU','SWISH','SELU'].index(activation)\n",
    "    print(\"Selected using \"+['ReLU','LeakyReLU','SWISH','SELU'][self.activation_method])\n",
    "\n",
    "\n",
    "  def forward_pass(self, x, w1, b1, w2, b2):\n",
    "    \"\"\"\n",
    "    A forward pass function\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out:\n",
    "        network output\n",
    "    cache:\n",
    "        intermediate values\n",
    "    \"\"\"\n",
    "    h1     = None  # the activation after the first linear layer\n",
    "    y1, y2 = None, None  # outputs from the first & second linear layers\n",
    "\n",
    "    #############################################################################\n",
    "    # PLACE YOUR CODE HERE                                                      #\n",
    "    #############################################################################\n",
    "    # TODO: Design the fully-connected neural network and compute its forward\n",
    "    #       pass output,\n",
    "    #        Input - Linear layer - LeakyReLU - Linear layer.\n",
    "    #       You have use predefined variables above\n",
    "\n",
    "    y1 =  \n",
    "\n",
    "\n",
    "      \n",
    "\n",
    "    y2 = \n",
    "\n",
    "    #  END OF YOUR CODE\n",
    "    #############################################################################\n",
    "\n",
    "    out   = y2\n",
    "    cache = (y1, h1) # intermediate values\n",
    "\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "  def softmax_loss(self, x, y):\n",
    "    \"\"\"\n",
    "    Compute the loss and gradients for a softmax classifier\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss:\n",
    "        the softmax loss\n",
    "    dx:\n",
    "        the gradient of loss\n",
    "\n",
    "    \"\"\"\n",
    "    # Compute the softmax classification loss and its gradient.                 #\n",
    "    # The softmax loss is also known as cross-entropy loss.                     #\n",
    "    p    = np.exp(x-np.max(x, axis=1, keepdims=True)) # for stable softmax\n",
    "    summ = np.sum(p, axis=1, keepdims=True)\n",
    "    p    /= summ\n",
    "    n    = len(x)\n",
    "    loss = -np.sum( np.log(p[range(n),y]) ) / n\n",
    "\n",
    "    dx = p.copy()\n",
    "    dx[range(n),y]-=1\n",
    "    dx /= n\n",
    "\n",
    "    return loss, dx\n",
    "\n",
    "\n",
    "  def backward_pass(self, dY2_dLoss, x, w1, y1, h1, w2):\n",
    "    \"\"\"\n",
    "    A backward pass function\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    grads:\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "\n",
    "    #############################################################################\n",
    "    # PLACE YOUR CODE HERE                                                      #\n",
    "    #############################################################################\n",
    "    # TODO: Compute the backward pass, computing the derivatives of the weights #\n",
    "    # and biases. Store the results in the grads dictionary. For example,       #\n",
    "    # the gradient on W1 should be stored in grads['w1'] and be a matrix of same#\n",
    "    # size                                                                      #\n",
    "\n",
    "    grads['w2'] = \n",
    "    grads['b2'] = \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "    grads['w1'] = \n",
    "    grads['b1'] = \n",
    "\n",
    "    #  END OF YOUR CODE\n",
    "    #############################################################################\n",
    "\n",
    "    return grads\n",
    "\n",
    "\n",
    "  def loss(self, x, y=None, regular=0.0, enable_margin=False):\n",
    "    \"\"\"\n",
    "    A loss function that returns the loss and gradients of the fully-connected\n",
    "    neural network. This function requires designing forward and backward passes.\n",
    "\n",
    "    If y is None, it returns a matrix labels of shape (n, c) where labels[i, c]\n",
    "    is the label score for class c on input x[i]. Otherwise, it returns a tuple\n",
    "    of loss and grads.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x:  matrix\n",
    "        an input data of shape (n, d). Each x[i] is a training sample.\n",
    "    y:  vector\n",
    "        a vector of training labels. Each y[i] is an integer in the range\n",
    "        0 <= y[i] < c. y[i] is the label for x[i]. If it is passed then we\n",
    "        return the loss and gradients.\n",
    "    regular: float\n",
    "        regularization strength.\n",
    "    enable_margin: Bool\n",
    "        enable to use soft-margin softmax\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss:\n",
    "        Loss (data loss and regularization loss) for this batch of training\n",
    "        samples.\n",
    "    grads:\n",
    "        Dictionary mapping parameter names to gradients of those parameters with\n",
    "        respect to the loss function; has the same keys as self.params.\n",
    "    \"\"\"\n",
    "    # Variables\n",
    "    n, d   = x.shape # input dimensions\n",
    "    w1, b1 = self.params['w1'], self.params['b1']\n",
    "    w2, b2 = self.params['w2'], self.params['b2']\n",
    "    h1     = None  # the activation after the first linear layer\n",
    "    y1, y2 = None, None  # outputs from the first & second linear layers\n",
    "\n",
    "    # Compute the forward pass\n",
    "    out, cache = self.forward_pass(x,w1,b1,w2,b2)\n",
    "    y2       = out\n",
    "    (y1, h1) = cache\n",
    "\n",
    "    # If the targets are not given then jump out, we're done\n",
    "    if y is None:\n",
    "      return y2\n",
    "\n",
    "    # Compute the loss\n",
    "    loss, dY2_dLoss = self.softmax_loss(y2, y)\n",
    "\n",
    "    # Compute the backward pass\n",
    "    grads = self.backward_pass(dY2_dLoss, x, w1, y1, h1, w2)\n",
    "\n",
    "    #############################################################################\n",
    "    # PLACE YOUR CODE HERE (OPTION: REGULARIZATION)                                     #\n",
    "    #############################################################################\n",
    "    # TODO: Implement weight regularization\n",
    "    #loss += \n",
    "    #loss += \n",
    "\n",
    "    #add regularization effect to the gradient terms\n",
    "    #grads['w2'] += \n",
    "    #grads['w1'] += \n",
    "    #  END OF YOUR CODE\n",
    "    #############################################################################\n",
    "\n",
    "    return loss, grads\n",
    "\n",
    "\n",
    "  def train(self, x, y, x_v, y_v,\n",
    "            eta=1e-3, lamdba=0.95,\n",
    "            regular=1e-5, num_iters=50,\n",
    "            batch_size=100, verbose=False):\n",
    "    \"\"\"\n",
    "    Train this neural network using stochastic gradient descent.\n",
    "\n",
    "    Inputs:\n",
    "    - x: A numpy array of shape (n, d) giving training data.\n",
    "    - y: A numpy array f shape (n,) giving training labels; y[i] = C means that\n",
    "      x[i] has label C, where 0 <= C < c.\n",
    "    - x_v: A numpy array of shape (n_v, d) giving validation data.\n",
    "    - y_v: A numpy array of shape (n_v,) giving validation labels.\n",
    "    - eta: Scalar giving learning rate for optimization.\n",
    "    - lamdba: Scalar giving factor used to decay the learning rate\n",
    "      after each epoch.\n",
    "    - regular: Scalar giving regularization strength.\n",
    "    - num_iters: Number of steps to take when optimizing.\n",
    "    - batch_size: Number of training examples to use per step.\n",
    "    - verbose: boolean; if true print progress during optimization.\n",
    "    \"\"\"\n",
    "    num_train = x.shape[0]\n",
    "    iterations_per_epoch = max(int(num_train / batch_size), 1)\n",
    "\n",
    "    # Use Stochastic Gradient Descent (SGD) to optimize the parameters in\n",
    "    # self.model\n",
    "    loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "\n",
    "    for it in range(num_iters):\n",
    "      x_batch = None\n",
    "      y_batch = None\n",
    "\n",
    "      #########################################################################\n",
    "      # PLACE YOUR CODE HERE                                                  #\n",
    "      #########################################################################\n",
    "      # TODO: Create a random minibatch of training data and labels, storing  #\n",
    "      # them in x_batch and y_batch respectively.                             #\n",
    "\n",
    "      \n",
    "      x_batch = \n",
    "      y_batch = \n",
    "\n",
    "      #  END OF YOUR CODE\n",
    "      #########################################################################\n",
    "\n",
    "      # Compute loss and gradients using the current minibatch\n",
    "      loss, grads = self.loss(x_batch, y=y_batch, regular=regular)\n",
    "      loss_history.append(loss)\n",
    "\n",
    "      #########################################################################\n",
    "      # PLACE YOUR CODE HERE                                                  #\n",
    "      #########################################################################\n",
    "      # TODO: Update the parameters of the network stored in self.params by   #\n",
    "      # using the gradients in the grads dictionary. For that, use stochastic #\n",
    "      # gradient descent.                                                     #\n",
    "\n",
    "      self.params['w1'] \n",
    "      self.params['w2'] \n",
    "      self.params['b1'] \n",
    "      self.params['b2'] \n",
    "\n",
    "      #  END OF YOUR CODE\n",
    "      #########################################################################\n",
    "\n",
    "      if verbose and it % 100 == 0:\n",
    "        print('The #iteration %d / %d: loss %f' % (it, num_iters, loss))\n",
    "\n",
    "      # Every epoch, check train and val accuracy and decay learning rate.\n",
    "      if it % iterations_per_epoch == 0:\n",
    "        # Check accuracy\n",
    "        train_acc = (self.predict(x_batch) == y_batch).mean()\n",
    "        val_acc = (self.predict(x_v) == y_v).mean()\n",
    "        train_acc_history.append(train_acc)\n",
    "        val_acc_history.append(val_acc)\n",
    "\n",
    "        # Decay learning rate\n",
    "        eta *= lamdba\n",
    "\n",
    "    return {\n",
    "      'loss_history': loss_history,\n",
    "      'train_acc_history': train_acc_history,\n",
    "      'val_acc_history': val_acc_history,\n",
    "    }\n",
    "\n",
    "  def predict(self, x):\n",
    "    \"\"\"\n",
    "    Use the trained weights of this MLP network to predict labels for\n",
    "    data points. For each data point we predict labels for each of the C\n",
    "    classes, and assign each data point to the class with the highest label\n",
    "    score.\n",
    "\n",
    "    Inputs:\n",
    "    - x: A numpy array of shape (n, d) giving n d-dimensional data points to\n",
    "      classify.\n",
    "\n",
    "    Returns:\n",
    "    - y_pr: A numpy array of shape (n,) giving predicted labels for each of\n",
    "      the elements of x. For all i, y_pred[i] = c means that x[i] is predicted\n",
    "      to have class C, where 0 <= C < c.\n",
    "    \"\"\"\n",
    "    y_pr = None\n",
    "\n",
    "    ###########################################################################\n",
    "    # PLACE YOUR CODE HERE                                                    #\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the predict function                                    #\n",
    "    out, _ = self.forward_pass(\n",
    "\n",
    "\n",
    "        \n",
    "                              )\n",
    "\n",
    "    y_pr = \n",
    "\n",
    "    # END OF YOUR CODE\n",
    "    ###########################################################################\n",
    "\n",
    "    return y_pr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xsvt-C2U5O-S"
   },
   "source": [
    "## Train a model\n",
    "To train our model, we will use a Stochastic Gradient Descent(SGD) method with momentum. In addition, we will adjust the learning rate with an exponential learning rate schedule as optimization proceeds; after each epoch, we will reduce the learning rate by multiplying it by a decay rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YOGURXt1qowf",
    "outputId": "6de7e7f6-5523-40c9-f030-66c13754ff04"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "input_size = 96 * 96 * 3\n",
    "hidden_size = 512\n",
    "num_classes = 10\n",
    "activation = 'ReLU' # Select one in [ReLU, LeakyReLU, SWISH, 'SELU']\n",
    "net_mlp = MLP(input_size, hidden_size, num_classes, activation)\n",
    "\n",
    "# Train the network\n",
    "stats = net_mlp.train(X_train, y_train, X_val, y_val,\n",
    "            num_iters=1000, batch_size=1024,\n",
    "            eta=1e-3, lamdba=0.99,\n",
    "            regular=0.001, verbose=True)\n",
    "\n",
    "# Predict on the validation set\n",
    "val_acc = (net_mlp.predict(X_val) == y_val).mean()\n",
    "print('Validation accuracy: ', val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zY_gDcL75pZP"
   },
   "source": [
    "## Visualization\n",
    "\n",
    "You have to plot the loss function and the accuracies on the training and validation sets. Then, visualize the weights that were learned in the first layer of the network. The weights of the intermediate layer may learn to represent specific features of the inputs, such as their curvature, thickness, or orientation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "ZBRyQO6tyTVf",
    "outputId": "5cefc153-3e3b-4d71-dd5b-a1c57a4ad5a1"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot the loss function and train / validation accuracies\n",
    "def showStats(stats):\n",
    "    fig, ax = plt.subplots(2, 1)\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(stats['loss_history'])\n",
    "    plt.title('Loss history')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    fig.tight_layout(pad=1.5)\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(stats['train_acc_history'], label='train')\n",
    "    plt.plot(stats['val_acc_history'], label='val')\n",
    "    plt.title('Classification accuracy history')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Clasification accuracy')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "showStats(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qhxfn83WyfAr"
   },
   "outputs": [],
   "source": [
    "def visualize_grid(xs, ubound=255.0, padding=1):\n",
    "  \"\"\"\n",
    "  Reshape an image data of 4D tensor to a grid for the better understanding and visualization.\n",
    "\n",
    "  Inputs:\n",
    "  - xs: Data of shape (n, h, w, c)\n",
    "  - ubound: Output grid will have values scaled to the range [0, ubound]\n",
    "  - padding: The number of blank pixels between elements of the grid\n",
    "  \"\"\"\n",
    "  (n, h, w, c) = xs.shape\n",
    "  grid_size = int(ceil(sqrt(n)))\n",
    "  grid_height = h * grid_size + padding * (grid_size - 1)\n",
    "  grid_width = w * grid_size + padding * (grid_size - 1)\n",
    "  grid = np.zeros((grid_height, grid_width, c))\n",
    "  next_idx = 0\n",
    "  y0, y1 = 0, h\n",
    "  for y in range(grid_size):\n",
    "    x0, x1 = 0, w\n",
    "    for x in range(grid_size):\n",
    "      if next_idx < n:\n",
    "        img = xs[next_idx]\n",
    "        low, high = np.min(img), np.max(img)\n",
    "        grid[y0:y1, x0:x1] = ubound * (img - low) / (high - low)\n",
    "        # grid[y0:y1, x0:x1] = Xs[next_idx]\n",
    "        next_idx += 1\n",
    "      x0 += w + padding\n",
    "      x1 += w + padding\n",
    "    y0 += h + padding\n",
    "    y1 += h + padding\n",
    "  # grid_max = np.max(grid)\n",
    "  # grid_min = np.min(grid)\n",
    "  # grid = ubound * (grid - grid_min) / (grid_max - grid_min)\n",
    "  return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "NR6BzQrmyfkE",
    "outputId": "2fb42fa1-1a5c-41f7-e588-5302873cf989"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from math import *\n",
    "\n",
    "def show_net_weights(net):\n",
    "  w1 = net_mlp.params['w1']\n",
    "\n",
    "  ###########################################################################\n",
    "  # PLACE YOUR CODE HERE                                                    #\n",
    "  ###########################################################################\n",
    "  # TODO: Implement the weight visualization\n",
    "\n",
    "\n",
    "    \n",
    "  plt.imshow(visualize_grid(w1, padding=3).astype('uint8'))\n",
    "  # END OF YOUR CODE\n",
    "  ###########################################################################\n",
    "\n",
    "  plt.gca().axis('off')\n",
    "  plt.show()\n",
    "\n",
    "show_net_weights(net_mlp)\n",
    "\n",
    "# Total parameters\n",
    "print(np.sum(np.shape(net_mlp.params['w1']))+\n",
    "      np.sum(np.shape(net_mlp.params['b1']))+\n",
    "      np.sum(np.shape(net_mlp.params['w2']))+\n",
    "      np.sum(np.shape(net_mlp.params['b2'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
